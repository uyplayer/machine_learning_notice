{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention（注意力）机制如果浅层的理解，跟他的名字非常匹配。他的核心逻辑就是「从关注全部到关注重点」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention（注意力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size,\n",
    "                 num_layers=1, bidirectional=False, batch_size=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers,\n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        directions = 2 if self.bidirectional else 1\n",
    "        return torch.zeros(\n",
    "            self.num_layers * directions,\n",
    "            self.batch_size,\n",
    "            self.hidden_size,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout_p=0):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=output_size,\n",
    "            embedding_dim=embedding_size\n",
    "        )\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "        # hc: [hidden, context]\n",
    "        self.Whc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # s: softmax\n",
    "        self.Ws = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        gru_out, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        attn_prod = torch.mm(self.attn(hidden)[0], encoder_outputs.t())\n",
    "        attn_weights = F.softmax(attn_prod, dim=1)\n",
    "        context = torch.mm(attn_weights, encoder_outputs)\n",
    "\n",
    "        # hc: [hidden: context]\n",
    "        hc = torch.cat([hidden[0], context], dim=1)\n",
    "        out_hc = F.tanh(self.Whc(hc))\n",
    "        output = F.log_softmax(self.Ws(out_hc), dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention（注意力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-21-9db717cd86f8>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-9db717cd86f8>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    return torch.zeros(1, 1, self.output_size,\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,hidden_size,bidirectional= True):\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,bidirectional = bidirectional)\n",
    "        \n",
    "        \n",
    "    def forward(self,input,hidden):\n",
    "        \n",
    "        output , hidden = self.lstm(input.view((1, 1, self.input_size)),hidden)\n",
    "        \n",
    "        return output,hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "         return (torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size),\n",
    "      torch.zeros(1 + int(self.bidirectional), 1, self.hidden_size))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class AttentionDecoder(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,input_size,output_size,vocab_size):\n",
    "        \n",
    "        super(AttentionDecoder,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.attn = nn.Linear(hidden_size + output_size, 1)\n",
    "        self.lstm = nn.LSTM(hidden_size + vocab_size, output_size)\n",
    "        self.final = nn.Linear(output_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "    return torch.zeros(1, 1, self.output_size,\n",
    "      torch.zeros(1, 1, self.output_size))    \n",
    "        \n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs, input):\n",
    "    \n",
    "    weights = []\n",
    "    for i in range(len(encoder_outputs)):\n",
    "      print(decoder_hidden[0][0].shape)\n",
    "      print(encoder_outputs[0].shape)\n",
    "      weights.append(self.attn(torch.cat((decoder_hidden[0][0], \n",
    "                                          encoder_outputs[i]), dim = 1)))\n",
    "    normalized_weights = F.softmax(torch.cat(weights, 1), 1)\n",
    "    \n",
    "    attn_applied = torch.bmm(normalized_weights.unsqueeze(1),\n",
    "                             encoder_outputs.view(1, -1, self.hidden_size))\n",
    "    \n",
    "    input_lstm = torch.cat((attn_applied[0], input[0]), dim = 1) #if we are using embedding, use embedding of input here instead\n",
    "    \n",
    "    output, hidden = self.lstm(input_lstm.unsqueeze(0), decoder_hidden)\n",
    "    \n",
    "    output = self.final(output[0])\n",
    "    \n",
    "    return output, hidden, normalized_weights       \n",
    "        \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c86a787aa4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbidirectional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Encoder' is not defined"
     ]
    }
   ],
   "source": [
    "bidirectional = True\n",
    "c = Encoder(10, 20, bidirectional)\n",
    "a, b = c.forward(torch.randn(10), c.init_hidden())\n",
    "print(a.shape)\n",
    "print(b[0].shape)\n",
    "print(b[1].shape)\n",
    "\n",
    "x = AttentionDecoder(20 * (1 + bidirectional), 25, 30)\n",
    "y, z, w = x.forward(x.init_hidden(), torch.cat((a,a)), torch.zeros(1,1, 30)) #Assuming <SOS> to be all zeros\n",
    "print(y.shape)\n",
    "print(z[0].shape)\n",
    "print(z[1].shape)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/translate/tree/master/pytorch_translate/attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,decoder_hidden_state_dim,context_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.decoder_hidden_state_dim = decoder_hidden_state_dim\n",
    "        self.context_dim = context_dim\n",
    "        \n",
    "    def forward(self,decoder_state, source_hids, src_lengths):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input\n",
    "            decoder_state: bsz x decoder_hidden_state_dim\n",
    "            source_hids: srclen x bsz x context_dim\n",
    "            src_lengths: bsz x 1, actual sequence lengths\n",
    "        Output\n",
    "            output: bsz x context_dim\n",
    "            attn_scores: max_src_len x bsz\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        # (B, L, H) -> (B , L, 1)\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
    "        # (B, L, H) * (B, L, 1) -> (B, H)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs, weights\n",
    "\n",
    "class AttnClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        \n",
    "    def set_embedding(self, vectors):\n",
    "        self.embedding.weight.data.copy_(vectors)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs, lengths):\n",
    "        batch_size = inputs.size(1)\n",
    "        # (L, B)\n",
    "        embedded = self.embedding(inputs)\n",
    "        # (L, B, E)\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "        out, hidden = self.lstm(packed_emb)\n",
    "        out = nn.utils.rnn.pad_packed_sequence(out)[0]\n",
    "        out = out[:, :, :self.hidden_dim] + out[:, :, self.hidden_dim:]\n",
    "        # (L, B, H)\n",
    "        embedding, attn_weights = self.attention(out.transpose(0, 1))\n",
    "        # (B, HOP, H)\n",
    "        outputs = self.fc(embedding.view(batch_size, -1))\n",
    "        # (B, 1)\n",
    "        return outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"    \n",
    "    scores each element of the sequence\n",
    "    with a linear layer and uses the normalized \n",
    "    scores to compute a context over the sequence.   \n",
    "    \"\"\"  \n",
    "    def __init__(self, d_hid, dropout=0.): \n",
    "        super().__init__()        \n",
    "        self.scorer = nn.Linear(d_hid, 1)       \n",
    "        self.dropout = nn.Dropout(dropout)   \n",
    "        \n",
    "    def forward(self, input_seq, lens):   \n",
    "        batch_size, seq_len, feature_dim = input_seq.size()   \n",
    "        input_seq = self.dropout(input_seq)      \n",
    "        scores = self.scorer(input_seq.contiguous().view(-1, feature_dim)).view(batch_size, seq_len)     \n",
    "        max_len = max(lens)      \n",
    "        for i, l in enumerate(lens):          \n",
    "            if l < max_len:               \n",
    "                scores.data[i, l:] = -np.inf      \n",
    "                scores = F.softmax(scores, dim=1)      \n",
    "        context = scores.unsqueeze(2).expand_as(input_seq).mul(input_seq).sum(1)      \n",
    "        return context # 既然命名为context就应该是整句的表示 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cloud.tencent.com/developer/article/1455815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a99e2ec46fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "        #inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "\n",
    "        QK = QK / (64**0.5)\n",
    "\n",
    "        QK = K.softmax(QK)\n",
    "\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "\n",
    "        V = K.batch_dot(QK,WV)\n",
    "\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "#标签转换为独热码\n",
    "y_train, y_test = pd.get_dummies(y_train),pd.get_dummies(y_test)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "#%%数据归一化处理\n",
    "\n",
    "maxlen = 64\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "#%%\n",
    "\n",
    "batch_size = 32\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers import *\n",
    "from Attention_keras import Attention,Position_Embedding\n",
    "\n",
    "\n",
    "S_inputs = Input(shape=(64,), dtype='int32')\n",
    "\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "\n",
    "\n",
    "O_seq = Self_Attention(128)(embeddings)\n",
    "\n",
    "\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(O_seq)\n",
    "\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "# try using different optimizers and different optimizer configs\n",
    "opt = Adam(lr=0.0002,decay=0.00001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(loss=loss,\n",
    "\n",
    "             optimizer=opt,\n",
    "\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#%%\n",
    "print('Train...')\n",
    "\n",
    "h = model.fit(x_train, y_train,\n",
    "\n",
    "         batch_size=batch_size,\n",
    "\n",
    "         epochs=5,\n",
    "\n",
    "         validation_data=(x_test, y_test))\n",
    "\n",
    "plt.plot(h.history[\"loss\"],label=\"train_loss\")\n",
    "plt.plot(h.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.plot(h.history[\"acc\"],label=\"train_acc\")\n",
    "plt.plot(h.history[\"val_acc\"],label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#model.save(\"imdb.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/4d52edda1d76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-71bca808dade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMulti30k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
